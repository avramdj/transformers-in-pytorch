## Transformers

### BERT

Paper: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

* Fill-mask language model pretraining for downstream tasks âœ…
* Sequence classification âœ…
* Token classification ðŸ’ 
* Next sentence prediction ðŸ’ 

### GPT-2

Paper: [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

* Semi-supervised training for sequence generation ðŸ’ 

### ViT

paper: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929.pdf)

* Image inpainting ðŸ’ 
* Image classification ðŸ’ 
